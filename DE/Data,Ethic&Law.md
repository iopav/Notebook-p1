### 2023年9月1日

```
1.What considerations are relevant when doing an ethical assessment of Sci-Hub?
2.Why are these relevant?
3.Who are relevant stakeholders and what are their interests?
4.Why are these relevant?
5.What arguments can you think of both for and against Sci-Hub's existence?
6.How can the interests between different stakeholders be balanced?
7.For instance, why should distribution of copyrighted scientific articles in the way that Sci-Hub does it be legalized and how would it affect different stakeholders?
```

#### Questions

1. Considerations relevant when doing an ethical assessment of Sci-Hub:

   - Copyright infringement
   - Access to knowledge
   - Funding of scientific research and publishing
   - Quality and reliability of research
   - Legal implications and compliance
   - Global equity in access to information
   - Role of open access movements

2. Why these considerations are relevant:

   - These factors raise ethical questions about intellectual property rights, equal access to knowledge, financial sustainability of research publishing, research quality, legal boundaries, global disparities, and the evolving norms in scholarly communication.

3. Relevant stakeholders and their interests:
   - Publishers (safeguarding copyright, financial sustainability)
   - Researchers (access to information, research quality)
   - Authors (copyright protection, recognition)
   - Students (affordable access to knowledge)
   - Institutions (research funding, reputation)
   - Legal entities (enforcement of copyright laws)
   - Advocates for open access (free knowledge dissemination)

   public wants to access to research output
   
4. Why these stakeholders are relevant:

   - Gains and outputs form a food chain in academic ecology

   - Their interests shape the ethical dimensions of Sci-Hub's existence and usage, encompassing copyright, knowledge access, research quality, legal compliance, funding, and scholarly communication.

5. Arguments for and against Sci-Hub's existence:

   **For Sci-Hub:**

   - Equitable access to knowledge for all, regardless of financial status.
   - Accelerates scientific progress by eliminating paywalls and barriers.
   - Bridges the global knowledge divide, fostering international collaboration.

   make a deal for lib

   **Against Sci-Hub:**

   - Copyright infringement and financial harm to publishers.
   - Undermines the peer-review process, risking research quality.
   - Sets a precedent for disregarding copyright law in the digital age.

   peer reviewing quality

6. Balancing interests between different stakeholders:
   - Promote open access models to make research widely available while respecting copyright.
   - Encourage publishers to adopt sustainable pricing models and support initiatives for affordable access.
   - Strengthen copyright laws while considering exceptions for educational and research purposes.
   - Support alternative peer-review models that maintain research quality.

7. Legalizing distribution of copyrighted scientific articles like Sci-Hub and its effects on stakeholders:

   Legalization Argument:
   - Equitable access to knowledge becomes a legal right, enhancing global education and research.
   - Researchers benefit from expanded access to relevant literature, accelerating their work.
   - Scientific progress and innovation are stimulated by unrestricted sharing of information.

   Effects on Stakeholders:
   - Positive impact on researchers, students, and institutions through improved access.
   - Publishers might adapt to open access models, potentially affecting revenue streams.
   - Authors could gain wider recognition as their work reaches a larger audience.
   - Legal authorities would need to oversee the distribution to ensure ethical compliance.

   Balancing:
   - Enforce proper regulations to prevent abuse of copyrighted material.
   - Establish mechanisms for compensating authors and publishers for the wider access to their work.
   - Collaborate with publishers and open access advocates to find sustainable solutions.

In this scenario, legalizing distribution as Sci-Hub does could address knowledge disparities, but it would require careful consideration of copyright, financial sustainability, and mechanisms to balance the interests of stakeholders involved.

```
How does Sci-Hub ensure the quality and validity of the data it provides? Does Sci-Hub have any mechanisms to detect and correct errors, frauds or retractions in the scientific papers it hosts? How does Sci-Hub deal with the ethical issues of plagiarism, authorship and citation in the data it disseminates?

```

#### generic drugs

[仿制药]是个通俗的说法，英文是[generic drug](https://www.zhihu.com/search?q=generic drug&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A852994408})。仿制药与对应的原研药（RLD，Reference Listed Drug）在活性药物成分（API，Active Pharmaceutical Ingredient）、剂型、规格上是一致的。但配方未必一致

我有一个可能会相关的想法，关于sci-hub与仿制药的联系。

- sci-hub在印度的案例与仿制药在印度的案例的相同点是：
  - 都涉及到印度对科学文献和药品的专利保护的问题，以及印度政府和法院如何平衡国际规则和国内利益的问题。
  - 都引起了国际社会的关注和争议，尤其是西方发达国家和跨国出版商或药企，他们认为印度侵犯了他们的知识产权，并要求印度加强专利保护和执行力度。
  - 都受到了印度国内和全球的支持和声援，包括科学家、医生、政策顾问、非政府组织等，他们认为印度的做法有利于促进科学研究和公共健康，为发展中国家提供了低价的文献和药物。
- sci-hub在印度的案例与仿制药在印度的案例的不同点是：
  - sci-hub是一个网站，它通过非法获取付费内容并免费提供给用户，而仿制药是一种产品，它通过逆向工程或其他方法复制原研药并以低价销售。
  - sci-hub是首次在法庭上为自己辩护，基于2012年德里大学教材复制案。
  - sci-hub作为一种虚拟主体，药品为实体主体，它是一种更为模糊的存在，它的道德边界并不像药品那么分明。所以这里还有很长的路要走。但我认为在数据日益重要的当代，数据流动的凝滞是一种慢性疾病，仿制药是治疗方法的一种预习。我们会打破这种壁垒，one way or another, 就像打破贸易壁垒一样。

如何改善科技不发达地区的困境，这是一个复杂而又重要的问题。我认为可能有以下一些方面可以考虑：

- 增加科技不发达地区的科研投入和人才培养，提高其自主创新能力和核心竞争力。
- 建立科技不发达地区之间的合作机制和交流平台，共享资源和经验，促进区域一体化和发展。
- 推动科技发达地区对科技不发达地区的支持和帮助，包括技术转让、知识共享、资金援助等。
- 倡导科技发达地区对科技不发达地区的尊重和理解，避免单方面强加规则或利用优势地位进行不公平竞争。
- 保护科技不发达地区的合法权益，维护其参与全球科技治理和决策的话语权。

胜利将表明'事实'只是一种意见

Victory will show the ‘fact’ to be merely an opinion



weigh the pros and cons

This brings me to the case of generics.

I have an idea that might be relevant, about the connection between sci-hub and generic drugs.

```
A generic drug is a medicine that has the same active ingredient and the same effects as a brand-name drug, but it is usually cheaper and sold under a different name.
```



- similarities :
  - Both involve the issue of India’s patent protection for papers and drugs, and the Indian government and courts found an interesting way to balance international rules and domestic interests. which is <u>*exemptions in India’s Copyright Act of 1957.*</u>
  
    ```
    Indian compulsory license is a legal authorization granted by the Indian Patent Office to a third party to use a patented invention without the consent of the patent owner, under certain conditions. The main purpose of compulsory license is to prevent the misuse of patent rights by the patent owner and to ensure the availability of affordable and accessible medicines for the public. 
    ```
  
    
  
  - Both have attracted the attention and controversy of the international community, especially the academic giants and multinational publishers or pharma<u>ceu</u>tical (su) companies, who believe that India has violated their intellectual property rights and demand India to strengthen patent protection and enforcement.
  
  - Both have received support and solidarity from within India and globally, including scientists, doctors, policy advisers, non-governmental organizations, etc., who believe that India’s actions are conducive to promoting scientific research and public health, and provide low-cost literature and drugs for developing countries.
- differences :
  - Sci-hub is a website that obtains paid content illegally and provides it to users for free, while generic drugs are a product that replicates original drugs by reverse engineering or other methods and sells them at a low price.
  - Sci-hub is defending itself in court for the first time, based on the 2012 Delhi University textbook copying case.
  - Sci-hub is a virtual entity, while drugs are a physical entity. It is a more ambiguous existence, and its moral boundaries are not as clear as drugs. So there is still a long way to go. But I think that in the contemporary era where data is increasingly important, data stagnation is a chronic disease, and generic drugs are a preview/rehearsal of the treatment method. We will break this barrier, one way or another, just like breaking trade barriers.

How to improve the plight of technologically underdeveloped areas is a complex and important issue. I think there may be some aspects to consider:

- Increase scientific research investment and talent cultivation in technologically underdeveloped areas, and improve their independent innovation ability and core competitiveness.
- Establish cooperation mechanisms and exchange platforms among technologically underdeveloped areas, share resources and experiences, and promote regional integration and development.
- Promote the support and help of technologically developed areas to technologically underdeveloped areas, including technology transfer, knowledge sharing, financial assistance, etc.
- Advocate respect and understanding of technologically underdeveloped areas by technologically developed areas, avoid imposing rules unilaterally or using advantageous positions for unfair competition.
- Protect the legitimate rights and interests of technologically underdeveloped areas, and maintain their voice in global science and technology governance and decision-making.

#### carbon emission zoning

```
different regions according to their carbon emission characteristics and emission reduction targets, so as to achieve inter-regional coordination and optimisation of carbon emission reduction
```

- Is it possible to partition academic output in the same way that carbon emissions are partitioned? 
- The two are different in terms of rights and obligations.
- Steam pricing is based on different countries.

#### thief steal

Despite being on the opposite side of the law, Sci-Hub is a tool that countless researchers rely on.

Sci-hub是一个提供免费学术论文下载的网站，它通过突破期刊的付费墙，获取了数千万篇受版权保护的文章，并向公众免费开放。Sci-hub的创始人Alexandra Elbakyan是一位哈萨克斯坦的计算机科学家，她认为科学知识应该是全人类的共同财富，而不是某些学术出版商的私有财产。

Sci-hub是否是学术窃贼，这是一个有争议的问题。一方面，学术出版商指责Sci-hub侵犯了他们的版权和利益，威胁了学术质量和安全，甚至将其称为“学术海盗”。他们在多个国家对Sci-hub提起了法律诉讼，试图关闭其网站和镜像。另一方面，许多科研工作者和学生支持Sci-hub的行为，认为它为他们提供了平等和便捷地获取学术资源的机会，促进了知识的传播和创新。他们认为学术出版商的收费过高，垄断了知识市场，剥削了科研者的劳动成果。

从语义上讲，偷走意味着原本拥有者失去了某物。但Sci-hub并没有使原本拥有者失去他们的知识，而只是复制了他们的知识，并将其分享给更多的人。因此，用偷是不合适的词汇。或许，用共享、传播、开放等词汇更能反映Sci-hub的真实性质。

Semantically, stealing implies that the original owner loses something. But Sci-hub does not cause the original owner to lose their knowledge, but merely copies it and shares it with more people. Therefore, stealing is not the right word to use. Perhaps the true nature of Sci-hub is better reflected by using terms such as sharing, dissemination, and openness.

最后，我想引用一句亚伦·斯沃茨（Aaron Swartz）的名言：“信息自由流动。” 他是一位互联网先驱和社会活动家，曾经为开源知识而奋斗，并因此遭到美国政府和学术出版商的迫害而自杀身亡。 他的故事也许可以给我们一些启示和灵感。

dig into the core deeper, Going further, we might talk about altruism

### 2023年9月6日



- **计算机化社会中的责任**：作者认为，随着计算机和计算机化系统对社会造成的危害和风险越来越多，责任感是一种重要的社会价值，但在我们的计算机化社会中却被系统地削弱了。
- **责任感的含义和价值**：作者解释了责任感的概念，即行为者对自己的行为进行解释和辩护的能力和义务。作者认为，责任感可以促进高质量的工作，鼓励勤奋、负责的行为，并为惩罚和赔偿提供基础。
- **责任感的缺失的原因和后果**：作者分析了导致责任感缺失的几个因素，包括对责任感的狭隘理解，对计算机系统能力和缺陷的错误假设，以及对计算机系统生产者不必对其产品的影响负全责的接受。作者指出，这种缺失意味着计算机是一种“失控”的技术，无法有效地应对其带来的问题。
- **恢复责任感的建议**：作者提出了一些建议，以增强计算机专业人员、计算机系统生产者和政府监管者对责任感的意识和承担。作者主张，应该建立更广泛、更深入、更灵活的责任感模式，以适应计算机化社会的复杂性和多样性。

### 2023年9月7日



- **计算与责任**：作者探讨了在计算机化社会中，责任感是如何被系统地削弱的，以及这对社会的价值有何影响。
- **责任感的重要性**：作者认为，责任感是一种宝贵的社会义务，它表明了对高质量工作的尊重，鼓励了勤奋、负责的行为，也为惩罚和赔偿提供了基础。
- **责任感的缺失**：作者分析了导致责任感缺失的几个因素，包括对责任感的狭隘理解，对计算机系统能力和缺陷的假设，以及对计算机系统生产者不必为其产品的影响负全责的接受。
- **责任感的恢复**：作者提出了一些建议，如建立道德标准，确保透明度和问责制，增强公众意识和教育，促进政府和行业之间的合作，以恢复计算机化社会中的责任感。



- **机器学习的法律责任**：本文探讨了由机器学习技术而非人类做出的决策所引发的法律责任问题，分析了可能涉及的损害赔偿和基本权利保护等方面。
- **严格责任和过失责任**：本文介绍了英国法律中两种可能适用于机器学习技术的责任制度，即无论有无过失都要承担责任的严格责任，以及要求被告有过失才能承担责任的过失责任。
- **责任人和责任范围**：本文分析了机器学习技术涉及的多方参与者，如技术生产者、技术使用者、技术受害者等，以及他们之间可能存在的法律关系和责任义务。
- **责任认定和因果关系**：本文讨论了机器学习技术在决策过程中可能出现的缺陷或错误，以及如何判断这些缺陷或错误是否构成了对他人的过失侵权，以及是否导致了他人的损害。



- **机器学习的法律责任**：本文探讨了由机器学习技术而非人类做出的决策所引发的法律责任问题，分析了可能涉及的损害赔偿和基本权利保护等方面。
- **严格责任和过失责任**：本文介绍了英国法律中两种可能适用于机器学习技术的责任制度，即无论有无过失都要承担责任的严格责任，以及要求被告有过失才能承担责任的过失责任。
- **责任人和责任范围**：本文分析了机器学习技术涉及的多方参与者，如技术生产者、技术使用者、技术受害者等，以及他们之间可能存在的法律关系和责任义务。
- **责任认定和因果关系**：本文讨论了机器学习技术在决策过程中可能出现的缺陷或错误，以及如何判断这些缺陷或错误是否构成了对他人的过失侵权，以及是否导致了他人的损害。





- **机器学习的法律责任问题**：本文分析了机器学习技术在决策过程中可能侵犯个人的基本权利或造成人身和财产损害的情况，以及如何确定责任人和责任范围。
- **严格责任和过失责任的适用**：本文介绍了两种可能的责任制度，一种是无需证明过失的严格责任，一种是需要证明过失的过失责任，讨论了它们在机器学习技术领域的优缺点和可行性。
- **责任认定和因果关系的判断**：本文指出了机器学习技术在解释自己的决策原理和过程方面的困难，以及这对于确定是否存在过失侵权和损害因果关系的影响。
- **暂缓监管，鼓励自律**：本文建议在机器学习技术发展初期，不要急于制定全面的法规，而是通过增加无法解释自己的机器学习技术的责任风险，来促使技术生产者建立道德标准和问责机制。



- **机器学习的解释**：作者介绍了一种基于反向传播的方法，称为层次相关性传播（LRP），可以解释非线性机器学习模型的预测。
- **机器学习的行为**：作者使用LRP分析了计算机视觉和街机游戏中的一些模型，发现它们展示了从简单到复杂的不同问题解决行为。
- **机器学习的评估**：作者提出了一种半自动化的方法，称为光谱相关性分析（SpRAy），可以在大规模数据集上检测模型的预测策略，并发现异常或不可接受的行为。
- **机器学习的意义**：作者认为，解释机器学习的决策过程是理解和验证模型是否具有有效和有用的问题解决能力的重要途径，也是评价机器智能的一个新维度。



- **神经影像学分析的可解释LSTM**：作者提出了一种基于层次相关性传播（LRP）的方法，可以解释长短期记忆网络（LSTM）在整个大脑神经影像学数据上的预测。
- **LSTM的优势和局限**：作者比较了LSTM和其他机器学习模型在神经影像学任务上的表现，发现LSTM具有更高的准确性和更好的泛化能力，但也存在一些缺点，如过拟合和不稳定性。
- **LRP的原理和应用**：作者介绍了LRP的原理，即通过反向传播将预测结果分配给输入特征，从而得到每个特征对预测的贡献。作者展示了LRP在三个神经影像学任务上的应用，分别是年龄预测、性别分类和情绪识别。
- **LRP的效果和意义**：作者评估了LRP的效果，发现它可以揭示LSTM在神经影像学分析中的内部机制，如特征选择、信息整合和时间依赖。作者认为，LRP可以提高LSTM的可解释性和可信度，也可以为神经科学研究提供新的视角。



##### **Q1: ** what

**What is the concept of responsibility for automated decisions in the context of data science?**

A1: Responsibility for automated decisions in data science refers to the ethical and moral obligations associated with the use of algorithms and machine learning models to make decisions that affect individuals, organizations, or society as a whole. It involves accountability for the consequences of these decisions and ensuring that they are fair, transparent, and unbiased.

reliability

keep rules with tech





##### **Q2: ** why

**Why is responsibility for automated decisions important in data science?**

A2: Responsibility is crucial in data science because automated decisions can have significant real-world impacts, including job hiring, loan approvals, and criminal justice. If not handled responsibly, these decisions can perpetuate bias, discrimination, and inequality. Responsible data science ensures that the technology is used to benefit society without harming vulnerable groups.

##### **Q3: ** where

**What are some ethical considerations in responsibility for automated decisions in data science?**

A3: Ethical considerations include:

1. **Bias and Fairness**: Ensuring that algorithms do not discriminate against certain groups based on race, gender, or other protected attributes.

2. **Transparency**: Making the decision-making process of algorithms transparent so that users can understand how decisions are reached.

3. **Accountability**: Identifying who is responsible for the decisions made by automated systems and holding them accountable for any negative outcomes.

4. **Data Privacy**: Protecting individuals' privacy and ensuring that personal data is used responsibly and securely.

##### **Q4: ** how

**How can responsibility for automated decisions be implemented in data science projects?**

A4: Implementing responsibility involves:

1. **Data Collection**: Ensuring that data used for training models is representative and free from bias.

2. **Algorithm Development**: Developing models that are fair, explainable, and accountable. Implementing fairness metrics and bias mitigation techniques.

3. **Transparency**: Documenting the decision-making process and making it understandable to non-technical stakeholders.

4. **Testing and Validation**: Rigorous testing to identify and mitigate biases and unintended consequences.

5. **Continuous Monitoring**: Ongoing monitoring to detect and address issues that may arise as the model is deployed.

##### **Q5:** challenge

**What are some challenges in achieving responsibility for automated decisions in data science? ** 

A5: Challenges include:

1. **Bias in Data**: Historical biases present in training data can be hard to eliminate.

2. **Algorithm Complexity**: Some algorithms, like deep neural networks, can be challenging to interpret and explain.

3. **Regulatory Compliance**: Complying with evolving data protection and fairness regulations.

4. **Balancing Trade-offs**: Balancing accuracy and fairness in model performance can be complex.

5. **Ethical Dilemmas**: Deciding on the appropriate level of autonomy for automated systems and handling moral dilemmas.

6. **Resource Constraints**: Small teams or limited budgets may hinder comprehensive responsible AI practices.

Achieving responsibility for automated decisions in data science is an ongoing effort that requires collaboration between data scientists, ethicists, policymakers, and other stakeholders.



##### ideas

```
管制者对车内的乘客、路上的行人之间的取舍，有非常复杂的利益关系。此时，无论管制者作出明确决策或者不做这个决策，都实际上构成了管制。让各个汽车公司自己去决定，这也是一种管制的决策。我们希望人们警醒，在管制语境中，各种商业利益和权力可能会伪装成技术、商业和政策上的某种必然性绑架决策过程。防止这一情况发生，需要有好的民主讨论和决策过程。
How do we prevent this from happening in an autonomous driving context? 
The controller has a very complex interest in the trade-off between the passengers in the car and the pedestrians on the road. At this point, whether the controller makes a clear decision or does not make that decision, it effectively constitutes regulation. 
Leaving it to the individual car companies to make their own decisions is also a kind of regulatory decision. We want people to be alerted to the fact that in a regulatory context, 

various commercial interests and powers may kidnap the decision-making process disguised as some sort of technical, commercial and policy inevitability. Preventing this from happening requires good democratic discussion and decision-making processes.


通过绑架流程来操纵决策，这一责任该由谁负？通常没有具体的人来负。或者决策完了没有正当的实施流程。技术是最终的执行者，但执行中可能的细节偏差往往缺乏专业而公正的鉴定，这就把"决策锅"变成了"执行锅"。
技术要不要发展这么快？在没有商业驱动的情况下，他们通常不打补丁。
Who is responsible for the manipulation of decision-making through the kidnapping process? Often there is no specific person to take responsibility. Or there is no proper implementation process after the decision is made. Technology is the ultimate implementer, but the implementation of the possible details of the deviation often lack of professional and impartial identification, which turns the "decision-making pot" into "implementation pot".
Should technology evolve so fast? In the absence of interests drivers, they usually not patched.

```





##### summary



We discussed about the concept of responsibility for automated decisions in data science, which is the ethical and moral obligation to ensure that algorithms and machine learning models are fair, transparent, and accountable for their impacts on people and society. We also discussed why responsibility is important in data science, as automated decisions can have significant real-world consequences, such as job hiring, loan approvals, and criminal justice. Then we listed some ethical considerations in responsibility for automated decisions, such as bias and fairness, transparency, accountability, and data privacy. 

We provided some examples of how to implement responsibility in data science projects, such as data collection, algorithm development, transparency, testing and validation, and continuous monitoring. Finally, we acknowledged some challenges in achieving responsibility for automated decisions, such as bias in data, algorithm complexity, regulatory compliance, balancing trade-offs, ethical dilemmas, and resource constraints. We concluded by stating that responsibility for automated decisions in data science is an ongoing effort that requires collaboration between different stakeholders.

### 2023年9月14日

- **Amazon’s sexist AI recruiting tool**: This article describes how Amazon developed an AI tool to evaluate job candidates’ resumes, but found that it showed bias against women.
- **The pitfalls of AI projects**: The article analyzes the possible causes of Amazon’s failure, such as vague business objectives, self-selection bias, keyword-based features, and inappropriate target and cost function.
- **The lessons learned from Amazon’s failure**: The article suggests that AI projects are complex and involve more than technical issues. It argues that data scientists and engineers need to work closely with business stakeholders and consider the ethical and social implications of their design decisions.



AI评估简历是否合理？

- 用ai评估简历，不等同于用ai评估人的价值。
- 用ai来评估人的是否适合这个岗位。是合理的。只是评估这个岗位和人是否匹配，是一个相关性。
- 只有人才能评价人，通过沟通来评价。这个ai只是一个初筛。用ai来筛选简历的初衷没有问题，但是结果不好，是ai训练的问题。初筛简历不是价值衡量，只是相关性匹配。
- 如果ai是一个得到公认的ai，那么它产出的评价label会成为大部分人认可的评价。

人的评价也有偏见，为什么AI的评价不能有偏见？

AI的评价会成为他人的参考，先入为主的印象，AI的影响力会随着AI的人为给予的可信度而迅速增长。但是人的评价是个体的，不会成为一个benchmark。

我们不该讨论这个ai本身的问题，而是讨论这个ai引发的问题。ai本身没有道德立场，我们利用ai的目的也是没有道德问题的，出发点是正确的。但是结果却又伦理偏差。这个问题的源头是数据输入的问题。

完美的数据集不存在

ai bias 就其本质，就是一个人与人之间的偏见问题。

我们不能因为 ai 模仿了人们的偏见而谴责ai，根源不在于算法，而在于人。

就算ai不惩罚女性简历，在接下来的筛选过程中，仍然无法保证她们不会因为性别而被惩罚。

ai撕破了求职市场表面公平的面纱，透露出其中带有偏见的本质。







Does it make sense for AI to evaluate resumes?

- Evaluating resumes with ai is not the same as evaluating people's value with ai.
- Using ai to assess the person's suitability for the position. It is reasonable. It is just a correlation assessing whether the position and the person are a good match.
- Only human can evaluate the value, through communication. This ai is just a preliminary screening. There is no problem with the original intent of using ai to screen CVs, but the bad results are a problem with the ai training. The initial screening of CVs is not a measure of value, just a relevance match.
- If the ai is widely recognized , then the evaluation labels it produces will be the ones most people recognize.

Human evaluations are also biased, why can't AI evaluations be biased?

AI's evaluation will become the baseline or benchmark of others, the preconceived impression, AI's influence will grow rapidly with AI's human given credibility. But human evaluation is individual and will not be a BENCHMARK.

it is not the problem of this AI itself, but the problem caused by this AI. ai itself has no ethical position, and our purpose of using ai is also no ethical problem, the starting point is correct. But the result is ethical bias. The source of this problem is the problem of data input.

Perfect data sets do not exist

ai bias is, by its very nature, a problem of interpersonal bias.

We can't condemn ai for performing people's biases; the root cause is not in the algorithm, but in the people.

Even if ai doesn't penalize women's CVs, there's still no guarantee that they won't be penalized on the basis of their gender in the subsequent screening process.

AI tears through the veil of apparent fairness in the job market and reveals the biased nature of it.



The widespread use of algorithms gives them a degree of publicness. Uniform ethical decisions magnify the impact of individual decisions. When algorithms from different fields are combined into an algorithmic system, the impact on individuals and society becomes global.

算法的广泛使用使其一定程度上具备了公共性。统一的伦理决定放大了个别决策的影响。当不同领域的算法组合成为一个算法系统时，对个人、社会的影响就变成全局性的了。

Why do seemingly neutral data samples instead derive discriminatory results? In fact, contrary to what most people think, **data in the context of AI is not completely objective and neutral, because both the collection, selection and use of data are selected and participated by people. In processing the data, engineers are required to make a variety of judgements, which are often subjective.** 

we will use an analogy to illustrate our main points. Imagine that AI is like a mirror that reflects our faces. If the mirror is distorted, dirty, or broken, it will not show us an accurate or flattering image of ourselves. Similarly, if the data and algorithms that train AI are biased, incomplete, or flawed, they will not produce fair or reliable results for us. Moreover, if we use the mirror as the only source of information about ourselves, we will miss out on other aspects of our identity and personality that the mirror cannot capture. Likewise, if we use AI as the only criterion for evaluating resumes, we will overlook other factors that may affect the suitability and potential of job candidates that AI cannot measure.

Understanding the ways in which humans are involved in the workings of AI is therefore essential. How to embed ethics in machines, what kind of ethical principles to embed, and what are the typical ethical issues are also of interest.
为什么看似中立的数据样本反而会导出歧视性结果？其实，与多数人的想象不同，人工智能语境下的数据并不完全客观中立，因为无论是数据的采集、选择和使用，均有人的挑选和参与。在对数据进行处理的过程中，工程师们需要作出各种判断，而这些判断常常是主观的。因此，了解人类以何种方式参与人工智能运作就显得十分必要。如何在机器中嵌入伦理、嵌入何种伦理原则、典型的伦理问题有哪些，也是我们要关注的内容。

#### summary

We discusses the ethical and social implications of using AI to evaluate resumes for job candidates. It argues that AI is not a neutral or objective tool, but rather a reflection of human biases and judgments that are embedded in the data and algorithms. 

AI can have a significant impact on individuals and society, especially when it is used as a benchmark or a public standard. We suggests that we should be critical and responsible for the creation, application, and impact of AI, and not rely on it as a substitute for human judgment. 

We are exploring the ethical and social implications of using AI to evaluate resumes for job candidates. We challenge the assumption that AI is a neutral or objective tool, but rather a reflection of human biases and judgments that are embedded in the data and algorithms. We also alert that AI can have a significant impact on individuals and society, especially when it is used as a benchmark or a public standard. We propose that we should be critical and responsible for the creation, application, and impact of AI, and not rely on it as a substitute for human judgment. We also raise some questions about how to embed ethics in machines, what kind of ethical principles to embed, and what are the typical ethical issues that arise from using AI to evaluate resumes.

we will use an analogy to illustrate our main points. Imagine that AI is like a mirror that reflects our faces. If the mirror is distorted, dirty, or broken, it will not show us an accurate or flattering image of ourselves. Similarly, if the data and algorithms that train AI are biased, incomplete, or flawed, they will not produce fair or reliable results for us. Moreover, if we use the mirror as the only source of information about ourselves, we will miss out on other aspects of our identity and personality that the mirror cannot capture. Likewise, if we use AI as the only criterion for evaluating resumes, we will overlook other factors that may affect the suitability and potential of job candidates that AI cannot measure.

Therefore, we suggest that we should not trust or reject AI blindly, but rather question its assumptions, methods, and outcomes. We should not blame or praise AI, but rather hold ourselves accountable for its creation, application, and impact. We should not rely or depend on AI, but rather use it as a supplement, not a substitute, for human judgment.

#### topic in slides

### 2023年9月21日

This paper suggests that the increased  interest in human factors among engineers reflects the irony that  the more advanced a control system is, so the more crucial may be  the contribution of the human operator. 

idea：

The refinement of the control system has stripped away unnecessary parts of the people management system, so that the remaining roles seem important because they were important in the first place, just wrapped up in an overcrowded system

 The  second irony is that the designer who tries to eliminate the  operator still leaves the operator to do the tasks which the  designer cannot think how to automate.

ques:

How can human operators maintain their manual and cognitive skills in highly automated systems, and what kind of training and practice do they need?

Is this really the brave new world we want?

Is there any dignity in this kind of work?

 advanced a control system 

Human beings would be considered as sensors of AI/CS in a highly automated society. For example, humans can provide feedback, input, or supervision to AI/CS systems, at the same time they can be manipulated, monitored, or influenced by AI/CS systems in a invisible way.

How can humans ensure that they are not deprived of their dignity by control systems?

```
AI systems should empower human dignity by enhancing human capabilities, skills, and opportunities. 
```



- One might argue that there is **no dignity** in this kind of work, because it involves **deskilling**, **monitoring**, and **replacing** human operators with automated systems. This may reduce the operator’s **status**, **motivation**, and **satisfaction**, and increase their **stress**, **workload**, and **error**. The operator may also lose their **manual** and **cognitive skills**, and become dependent on or distrustful of the computer. The operator may feel that they have no meaningful role or contribution in the system, and that they are not respected or valued by the designers or managers.
- Another might argue that there is **some dignity** in this kind of work, because it still requires the operator to have **knowledge**, **judgement**, and **responsibility** for the system. The operator may still be involved in **supervising**, **adjusting**, **maintaining**, or **improving** the system, or in dealing with **abnormal** or **emergency** situations. The operator may also benefit from the **support** and **aiding** of the computer, which can enhance their **capabilities**, **performance**, and **safety**. The operator may feel that they have a complementary role or collaboration with the computer, and that they are recognized or rewarded by the designers or managers.

Ultimately, the dignity of this kind of work may depend on how the automation is designed, implemented, and evaluated, and how the human factors are considered and addressed. Some possible ways to improve the dignity of this kind of work are:

- Developing and applying ethical principles and guidelines for the automation of industrial processes.
- Adopting a human-centered approach to automation, which puts human dignity at the core of AI ethics and governance, and ensures that AI systems are aligned with human values and rights, and serve the common good of humanity.
- Designing better interfaces and displays for human-computer collaboration, which respect and promote the diversity, preferences, and goals of human operators, and facilitate their participation, empowerment, and feedback.
- Providing adequate training and practice for human operators to maintain their manual and cognitive skills, and to cope with complexity, uncertainty, and change in automated systems.
- Enhancing the job content and quality of human operators by enriching their tasks, increasing their autonomy and control, and offering them opportunities for learning and development.

#### summary



The content discusses the **ironies** and **challenges** of automation in industrial processes, and how it affects the role and dignity of human operators. It argues that automation can have both **positive** and **negative** impacts on human operators, depending on how it is designed, implemented, and evaluated. It also raises some **questions** about how human operators can maintain their skills, cope with complexity, and ensure their dignity in highly automated systems. It suggests some possible ways to improve the human-computer collaboration, such as ethical principles, human-centered approach, better interfaces, adequate training, and job enrichment.





- It discusses the problems and challenges of automation in industrial processes, and how it affects the role and dignity of human operators.
- argues that automation can have both positive and negative impacts on human operators, depending on how it is designed, implemented, and evaluated. It also raises some questions about how human operators can maintain their skills, cope with complexity, and ensure their dignity in highly automated systems.
-  identifies two main ironies of automation:
  - The first irony is that the system designers may view the human operator as unreliable and inefficient, and try to eliminate them from the system. However, this may result in leaving the operator with the tasks that are too difficult or unpredictable to automate, and without adequate support or training for them.
  - The second irony is that the system designers may expect the human operator to monitor and take over the automated system when it fails. However, this may require the operator to have more skill and knowledge than before, which may deteriorate due to lack of practice and feedback.
- suggests some possible solutions to these problems, such as:
  - Providing artificial assistance and alarms for monitoring the automated system and detecting failures.
  - Allowing the operator to use manual control for a short period in each shift or on a simulator to maintain their manual and cognitive skills.
  - Developing ethical principles and guidelines for automation of industrial processes.
  - Adopting a human-centered approach to automation, which puts human dignity at the core of AI ethics and governance.
  - Designing better interfaces and displays for human-computer collaboration, which respect and promote the diversity, preferences, and goals of human operators.
- also introduces some aspects of human-computer collaboration in on-line control, such as:
  - Giving instructions or advice to the operator, or mitigating their errors.
  - Providing software-generated displays that are compatible with different types of operator skill and task complexity.
  - Relieving human workload by adapting computer aiding to human attention and performance.

 It suggests some possible ways to improve the human-computer collaboration, such as ethical principles, human-centered approach, better interfaces, adequate training, and job enrichment.

