{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "14th of January 2020 \n",
    "根据当前页面的内容，这里是一些关于Python代码补充的建议：\n",
    "\n",
    "- **问题1（矩阵奇异值分解）**:\n",
    "  - 检查导入语句，应使用`import numpy as np`来导入NumPy库。\n",
    "  - 使用`np.linalg.svd()`函数来计算矩阵M的奇异值分解。\n",
    "  - 确保变量`U`, `S`, `Vt`正确存储了奇异值分解的结果。\n",
    "\n",
    "- **问题2（Wald检验）**:\n",
    "  - 计算正态分布的最大似然估计（MLE），可以使用`np.mean()`和`np.var()`函数。\n",
    "  - 根据Wald检验公式计算统计量W。\n",
    "  - 使用`scipy.stats.norm.ppf()`函数来找到临界值，并判断是否拒绝零假设。\n",
    "\n",
    "- **问题3（残差图和Wald检验）**:\n",
    "  - 使用`plt.scatter()`函数来绘制残差图。\n",
    "  - 计算线性回归模型的系数，可以使用`np.polyfit()`或`scipy.linalg.lstsq()`。\n",
    "  - 对于Wald检验，计算回归系数的标准误并使用标准正态分布的临界值。\n",
    "\n",
    "- **问题4（马尔可夫链）**:\n",
    "  - 使用字符串方法`split()`来将文本分割成单词列表。\n",
    "  - 创建一个字典来计算每个单词转移到另一个单词的频率。\n",
    "  - 根据频率计算转移概率矩阵。\n",
    "\n",
    "- **问题5（统计问题）**:\n",
    "  - 对于球体表面的随机点问题，使用积分或几何概率方法来解决。\n",
    "  - 对于体积问题，可能需要使用积分和极坐标变换。\n",
    "  - 利用斯特林公式来近似阶乘，从而解决半径问题。\n",
    "\n",
    "- **问题6（感知机算法）**:\n",
    "  - 实现感知机算法，更新权重向量直到所有数据点正确分类。\n",
    "  - 计算数据点到决策边界的最小距离，以估计算法的迭代次数上限。\n",
    "\n",
    "- **问题7（自举法）**:\n",
    "  - 使用`np.percentile()`函数来计算地震间隔时间的百分位数。\n",
    "  - 实现自举法来估计统计量的置信区间。\n",
    "\n",
    "- **问题8（最大似然估计）**:\n",
    "  - 定义负对数似然函数。\n",
    "  - 使用`scipy.optimize.minimize_scalar()`函数来找到最大似然估计。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "5"
   },
   "source": [
    "---\n",
    "## PROBLEM 1\n",
    "Maximum Points = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "5"
   },
   "source": [
    "\n",
    "Consider the following matrix\n",
    "$$\n",
    "    M = \n",
    "    \\begin{bmatrix}\n",
    "        1 & 1 \\\\\n",
    "        0 & 3 \\\\\n",
    "        3 & 0\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "Manually, by hand, produce the\n",
    "* [1p] Rank: `rank`\n",
    "* [1p] Left singular vectors in matrix form: `V`\n",
    "* [1p] Singular values in matrix form: `D`\n",
    "* [1p] Right singular vectors in matrix form: `U`\n",
    "\n",
    "If $UDV^T = M$ then [1p].\n",
    "\n",
    "When answering these questions, use the sagemath `matrix` format as the example below. Use exact answers, i.e. use `sqrt(3)` if that appears in the calculation.\n",
    "\n",
    "> To make sure that we all agree on signs, make sure that the sign of the first component of each singular vector is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msagemath\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m M \u001b[38;5;241m=\u001b[39m \u001b[43mMatrix\u001b[49m([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m],[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m],[\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m0\u001b[39m]])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Matrix' is not defined"
     ]
    }
   ],
   "source": [
    "import sagemath\n",
    "M = matrix([[1,1],[0,3],[3,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "5"
   },
   "outputs": [],
   "source": [
    "rank = 1 # [1p] The rank of the matrix M\n",
    "# V = XXX # [1p] Matrix of singular vectors, each vector is a column of V\n",
    "# D = XXX # [1p] The square matrix with the singular values on the diagonal\n",
    "# U = XXX # [1p] The matrix of right singular vectors (each vector is a column of U)\n",
    "import numpy as np\n",
    "\n",
    "# 定义矩阵\n",
    "M = np.array([[1, 1],\n",
    "              [0, 3],\n",
    "              [3, 0]])\n",
    "\n",
    "# 进行奇异值分解\n",
    "\n",
    "U, S, Vt = np.linalg.svd(M)\n",
    "print(U,S,Vt)\n",
    "# Constructing the diagonal matrix D from the singular values S\n",
    "D_adjusted = np.zeros((U.shape[0], V.shape[1]))\n",
    "D_adjusted[:S.shape[0], :S.shape[0]] = np.diag(S)\n",
    "# Ensuring the first component of each singular vector is positive\n",
    "for i in range(len(U[0])):\n",
    "    if U[0, i] < 0:\n",
    "        U[:, i] *= -1\n",
    "for i in range(len(V[0])):\n",
    "    if V[0, i] < 0:\n",
    "        V[:, i] *= -1\n",
    "\n",
    "# Verification that UDV^T = M\n",
    "#UDVT = U @ D_adjusted @ V.T\n",
    "# Outputs\n",
    "\n",
    "V = Vt.T # [1p] Matrix of singular vectors, each vector is a column of V\n",
    "D = D_adjusted # [1p] The square matrix with the singular values on the diagonal\n",
    "U = U # [1p] The matrix of right singular vectors (each vector is a column of U)\n",
    "## Hint, use V.nrows() and V.ncols() etc. to make sure that you got the dimensions as you\n",
    "## expect. If nrows does not work, then you should make sure you are using `matrix()` from sagemath\n",
    "\n",
    "## [1p] Hint: Check that U*D*V.transpose() == M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "5"
   },
   "source": [
    "---\n",
    "## PROBLEM 2\n",
    "Maximum Points = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "5"
   },
   "source": [
    "\n",
    "Consider the $n$ IID $\\theta$-parametric family of rescaled Rademacher random variables with the following probability mass function:\n",
    "\n",
    "$$\n",
    "f(x; \\theta) = \n",
    "\\begin{cases} \n",
    "\\theta & \\text{ if } x=+10\\\\\n",
    "1-\\theta & \\text{ if } x=-10\\\\\n",
    "0 & \\text{ otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Your task now is to perform a Wald Test of size $\\alpha=0.05$ to try to reject the null hypothesis that the chance of seeing a $+10$ is exactly $1/2$, i.e.,\n",
    "$$\\displaystyle{H_0: \\theta^*=\\theta_0 \\quad \\text{ versus } \\quad H_1: \\theta^* \\neq \\theta_0, \\qquad \\text{ with }\\theta_0=0.5}$$\n",
    "Show you work by replacing `XXX`s with the right expressions in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dataSamples2 = np.array([-10,+10,+10,-10,-10,-10,+10,+10,-10,-10,-10,+10,+10,-10,+10,+10,+10])\n",
    "# HINT: to get the counts of +10s and -10s, perhaps a useful statistic for the likelihood\n",
    "print (sum(dataSamples2==-10)) # number of -10s\n",
    "print (sum(dataSamples2==+10)) # number of +10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mle thetaHat =  0.5294117647058824\n",
      "mle thetaHat =  0.5294117647058824\n"
     ]
    }
   ],
   "source": [
    "thetaHat=sum(dataSamples2 == +10) / len(dataSamples2)\n",
    "print (\"mle thetaHat = \",thetaHat)\n",
    "thetaHat= np.mean(dataSamples2 == 10)\n",
    "print (\"mle thetaHat = \",thetaHat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "5"
   },
   "outputs": [],
   "source": [
    "## HINT: Think how the likelihood here is related to that for Bernoulli trials\n",
    "\n",
    "## STEP 1: get the MLE thetaHat, \n",
    "### either by hand or numerically\n",
    "thetaHat=sum(dataSamples2 == +10) / len(dataSamples2)\n",
    "print (\"mle thetaHat = \",thetaHat)\n",
    "\n",
    "## STEP 2: get the NullTheta or theta0\n",
    "NullTheta=0.5\n",
    "print (\"Null value of theta under H0 = \", NullTheta)\n",
    "\n",
    "## STEP 3: get estimated standard error\n",
    "seTheta=np.sqrt(NullTheta * (1 - NullTheta) / len(dataSamples2))\n",
    "# recall standard error comes from Fisher Information\n",
    "print (\"estimated standard error\",seTheta)\n",
    "\n",
    "# STEP 4: get Wald Statistic\n",
    "W=(thetaHat - NullTheta) / seTheta\n",
    "\n",
    "print (\"Wald statistic = \",W)\n",
    "\n",
    "# STEP 5: conduct the size alpha=0.05 Wald test\n",
    "# do NOT change anything below\n",
    "rejectNull1 = abs(W) > 2.0 # alpha=0.05, so z_{alpha/2} =1.96 approx=2.0\n",
    "if (rejectNull1):\n",
    "    print (\"we reject the null hypothesis that theta_0=0.5\")\n",
    "else:\n",
    "    print (\"we fail to reject the null hypothesis that theta_0=0.5\")\n",
    "    \n",
    "    \n",
    "# dataSamples2 = np.array([-10, +10, +10, -10, -10, -10, +10, +10, -10, -10, -10, +10, +10, -10, +10, +10, +10])\n",
    "# theta_hat = np.mean(dataSamples2 == +10)\n",
    "# se_theta = np.sqrt((theta_hat * (1 - theta_hat)) / len(dataSamples2))\n",
    "# W = (theta_hat - 0.5) / se_theta\n",
    "# reject_null = abs(W) > 2.0\n",
    "\n",
    "# print(\"MLE thetaHat:\", theta_hat)\n",
    "# print(\"Null value of theta under H0:\", 0.5)\n",
    "# print(\"Estimated standard error:\", se_theta)\n",
    "# print(\"Wald statistic:\", W)\n",
    "# print(\"Reject the null hypothesis:\" if reject_null else \"Fail to reject the null hypothesis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "5"
   },
   "source": [
    "---\n",
    "## PROBLEM 3\n",
    "Maximum Points = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "5"
   },
   "source": [
    "\n",
    "In the next cells the earthquake data is analyzed further, specifically depth and magnitude.\n",
    "\n",
    "Your task is to understand the analysis and continue with the following:\n",
    "\n",
    "1. Make a residual plot for the fit and discuss briefly the scatter of residuals. Explain what values that are farthest from the x-axis (both below and above) mean here.\n",
    "2. Conduct a Wald test with alpha at 5% of the null hypothesis that $\\beta_1=0$. State your conclusion by setting the Boolean variable `RejectNullHypothesisForProblem4` to `True` if you reject and `False` if you do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "5"
   },
   "outputs": [],
   "source": [
    "# Lets extract the depth and magnitude from myProcessedList \n",
    "# (which contains: longitude, latitude, magnitude, depth and the origin time)\n",
    "eqData = np.array(myProcessedList)[:,[3,2]]\n",
    "eqDepth = eqData[:,0]\n",
    "eqMagnitude = eqData[:,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "5"
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import lstsq\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "M1 = eqDepth[:, np.newaxis]^[0, 1]\n",
    "b, res, rnk, s = lstsq(M1, eqMagnitude)\n",
    "plt.plot(eqDepth, eqMagnitude, 'o', label='data')\n",
    "\n",
    "xx = np.linspace(0.0, 550, 101)\n",
    "yy = b[0] + b[1]*xx \n",
    "plt.plot(xx, yy, label='least squares fit')\n",
    "plt.xlabel('Earthquake Depth (X)')\n",
    "plt.ylabel('Earthquake Magnitude (Y)')\n",
    "plt.legend(framealpha=1, shadow=True)\n",
    "plt.grid(alpha=0.25)\n",
    "plt.text(3, 8.5, r'$\\widehat{r}(x) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x, \\quad \\\n",
    "\\widehat{\\beta}_0 = $ %(b0)0.3f , $\\widehat{\\beta}_1 = $ %(b1)0.3f' % {'b0': b[0], 'b1': b[1]} )\n",
    "plt.show()\n",
    "\n",
    "\n",
    "############\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设eqDepth和eqMagnitude是从数据中提取的地震深度和震级\n",
    "# eqDepth = ...\n",
    "# eqMagnitude = ...\n",
    "\n",
    "# 使用最小二乘法拟合线性模型\n",
    "b, res, rnk, s = np.linalg.lstsq(eqDepth[:, np.newaxis], eqMagnitude, rcond=None)\n",
    "residuals = eqMagnitude - (b[0] + b[1] * eqDepth)\n",
    "plt.scatter(eqDepth, residuals)\n",
    "plt.xlabel('Earthquake Depth')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "# Wald Test\n",
    "beta_hat = b[1]\n",
    "se_beta = np.sqrt(res / (len(eqDepth) - 2)) / np.std(eqDepth)\n",
    "W = beta_hat / se_beta\n",
    "reject_null_hypothesis_for_problem4 = abs(W) > 2.0\n",
    "\n",
    "print(\"Beta hat:\", beta_hat)\n",
    "print(\"Standard error:\", se_beta)\n",
    "print(\"Wald statistic:\", W)\n",
    "print(\"Reject the null hypothesis for beta_1 = 0:\" if reject_null_hypothesis_for_problem4 else \"Fail to reject the null hypothesis for beta_1 = 0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/earthquakes_small.csv\")\n",
    "myProcessedList = data\n",
    "eqDepth = data[' depth'].values\n",
    "eqMagnitude = data[' magnitude'].values\n",
    "\n",
    "from scipy.linalg import lstsq\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "M1 = np.column_stack((np.ones(eqDepth.shape), eqDepth))\n",
    "\n",
    "b, res, rnk, s = lstsq(M1, eqMagnitude)\n",
    "\n",
    "# Re-plotting the data and the fit\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(eqDepth, eqMagnitude, 'o', label='Data')\n",
    "xx = np.linspace(min(eqDepth), max(eqDepth), 101)\n",
    "yy = b[0] + b[1] * xx\n",
    "plt.plot(xx, yy, label='Least Squares Fit')\n",
    "plt.xlabel('Earthquake Depth (X)')\n",
    "plt.ylabel('Earthquake Magnitude (Y)')\n",
    "plt.legend(framealpha=1, shadow=True)\n",
    "plt.grid(alpha=0.25)\n",
    "plt.title('Earthquake Depth vs Magnitude')\n",
    "plt.text(min(eqDepth), max(eqMagnitude), r'$\\widehat{r}(x) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x, \\quad \\widehat{\\beta}_0 = $' + f'{b[0]:0.3f}' + r', $\\widehat{\\beta}_1 = $' + f'{b[1]:0.3f}')\n",
    "plt.show()\n",
    "\n",
    "# Calculating residuals for the residual plot\n",
    "residuals = eqMagnitude - (b[0] + b[1] * eqDepth)\n",
    "b, residuals   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "3"
   },
   "source": [
    "\n",
    "### Part 1. Do a residual analysis by creating a cell below \n",
    "\n",
    "You should make a plot of the residuals from the fitted model above and explain your findings between the two `---` lines in this cell.\n",
    "\n",
    "After performing linear least squares fitting on the data of earthquake depth and magnitude, we obtained the following results:\n",
    "\n",
    "The estimated coefficients are (\\widehat{\\beta}_0 = 2.045) and (\\widehat{\\beta}_1 = 0.0033).\n",
    "Residual Plot Analysis\n",
    "The residual plot shows the difference between the observed magnitudes and those predicted by the linear model at each earthquake depth.\n",
    "\n",
    "Interpretation of the Residual Plot:\n",
    "Scatter of Residuals: Ideally, the residuals should be randomly distributed around the horizontal line at 0, indicating a good fit. Any systematic patterns in the residuals suggest a potential issue with the model, such as non-linearity or heteroscedasticity.\n",
    "Values Far from the X-Axis: Residuals farthest from the x-axis (both above and below) represent data points where the model's predictions are most inaccurate. A large positive residual indicates that the actual magnitude was significantly higher than the model's prediction, whereas a large negative residual indicates that the actual magnitude was much lower than predicted.\n",
    "Wald Test for (\\beta_1=0)\n",
    "A Wald test was conducted with an alpha of 5% to test the null hypothesis that (\\beta_1=0). The calculated Wald statistic is approximately (4.84), which is greater than the critical value of approximately (1.96) for a significance level of 5%.\n",
    "\n",
    "Conclusion:\n",
    "Since the Wald statistic exceeds the critical value, we reject the null hypothesis that (\\beta_1=0). This suggests that there is a statistically significant linear relationship between earthquake depth and magnitude at the 5% significance level. In the context of this analysis, the Boolean variable RejectNullHypothesisForProblem4 is set to True, indicating that the null hypothesis is rejected.\n",
    "\n",
    "地震深度与震级数据分析\n",
    "在对地震深度与震级的数据进行线性最小二乘拟合后，我们得到了以下结果：\n",
    "\n",
    "估计的系数为 (\\widehat{\\beta}_0 = 2.045) 和 (\\widehat{\\beta}_1 = 0.0033)。\n",
    "残差图分析\n",
    "残差图显示了模型在每个地震深度处的预测与观测震级之间的差异。\n",
    "\n",
    "残差图的解释：\n",
    "残差的散布情况：理想情况下，残差应该围绕在0的水平线周围随机分布，这表明拟合效果良好。残差中的任何系统性模式都表明模型可能存在问题，比如非线性或异方差性。\n",
    "远离X轴的值：最远离x轴的残差（无论是上方还是下方）表示模型预测最不准确的数据点。正的大残差表明实际震级显著高于模型的预测，而负的大残差表明实际震级远低于预测值。\n",
    "对 (\\beta_1=0) 的 Wald 检验\n",
    "进行了Wald检验，以5%的显著性水平测试 (\\beta_1=0) 的零假设。计算得出的Wald统计量约为 (4.84)，高于5%显著性水平下的临界值约 (1.96)。\n",
    "\n",
    "结论：\n",
    "由于Wald统计量超过了临界值，我们拒绝零假设 (\\beta_1=0)。这表明在5%的显著性水平下，地震深度与震级之间存在统计学上的显著线性关系。在这项分析的背景下，布尔变量 RejectNullHypothesisForProblem4 被设定为 True，表示拒绝了零假设。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Part 2. Do a Wald Test of H0: beta_1 = 0\n",
    "# show your working here\n",
    "# write down your conclusion by replacing XXX in next line\n",
    "# You will not get points for randomly guessing True/False\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Wald Test\n",
    "alpha = 0.05  # Significance level\n",
    "\n",
    "# Standard error of beta_1 (from least squares regression)\n",
    "se_beta_1 = np.sqrt(res / (len(eqDepth) - 2)) / np.sqrt(np.sum((eqDepth - np.mean(eqDepth))**2))\n",
    "\n",
    "# Wald statistic\n",
    "W = b[1] / se_beta_1\n",
    "\n",
    "# Critical value from standard normal distribution for alpha = 0.05\n",
    "critical_value = norm.ppf(1 - alpha / 2)\n",
    "\n",
    "# Conclusion\n",
    "RejectNullHypothesisForProblem4 = abs(W) > critical_value\n",
    "\n",
    "print(W, critical_value, RejectNullHypothesisForProblem4)\n",
    "\n",
    "# write down your conclusion by replacing XXX in next line\n",
    "# You will not get points for randomly guessing True/False\n",
    "RejectNullHypothesisForProblem4 = RejectNullHypothesisForProblem4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "4",
    "lx_problem_points": "5"
   },
   "source": [
    "---\n",
    "## PROBLEM 4\n",
    "Maximum Points = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "4",
    "lx_problem_points": "5"
   },
   "source": [
    "1. Take the string `prideAndPrejudiceFirstChapter` and split it by `' '` into a list of \"words\" and put this in `words`.\n",
    "2. Consider the list of words as a list of states, precisely as in the `wet` - `dry` Markov chain that we studied. We model this list of states using a Markov chain, as such there is an associated transition matrix $P$. The first four words are `['it', 'is', 'a', 'truth']`, if we think of this as a Markov chain  we will have transitions from `'it'` to `'is'` for instance, as such there is a $p_{\\textrm{'it','is'}}$, i.e. a transition probability from `'it'` to `'is'`.\n",
    "3. Your goal is to find the maximum likelihood estimate of $P$, recall from notebook 13 that for two states we have\n",
    "$$\n",
    "    \\widehat{p}_{0,0} = \\frac{n_{0,0}}{n_{0,0}+n_{0,1}} \n",
    "    \\quad \\text{and} \\quad \n",
    "    \\widehat{p}_{1,1} = \\frac{n_{1,1}}{n_{1,0}+n_{1,1}}\n",
    "$$ \n",
    "there is nothing special about two states, so for arbitrary number of states $i=1,\\ldots,N$ we have\n",
    "$$\n",
    "    \\widehat{p}_{i,j} = \\frac{n_{i,j}}{\\sum_{k=1}^N n_{i,k}} \n",
    "$$\n",
    "4. The order of the indices should be the same as the list `unique_words` i.e. the first word in that list corresponds to $i=0$, the second $i=1$ etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "4",
    "lx_problem_points": "5"
   },
   "outputs": [],
   "source": [
    "# REQUIRED-CELL\n",
    "# DO NOT MODIFY this cell\n",
    "# Evaluate this cell before trying this PROBLEM so that the required functions and variables are loaded\n",
    "def makeFreqDict(myDataList):\n",
    "    '''Make a frequency mapping out of a list of data.\n",
    "    \n",
    "    Param myDataList, a list of data.\n",
    "    Return a dictionary mapping each unique data value to its frequency count.'''\n",
    "       \n",
    "    freqDict = {} # start with an empty dictionary\n",
    "        \n",
    "    for res in myDataList:\n",
    "        \n",
    "        if res in freqDict: # the data value already exists as a key\n",
    "                freqDict[res] = freqDict[res] + 1 # add 1 to the count using sage integers\n",
    "        else: # the data value does not exist as a key value\n",
    "            freqDict[res] = 1 # add a new key-value pair for this new data value, frequency 1\n",
    "        \n",
    "    return freqDict # return the dictionary created\n",
    "\n",
    "# end of makeFreqDict(...)\n",
    "\n",
    "prideAndPrejudiceFirstChapter = '''It is a truth universally acknowledged, that a single man in\n",
    "      possession of a good fortune, must be in want of a wife.\n",
    "\n",
    "      However little known the feelings or views of such a man may be\n",
    "      on his first entering a neighbourhood, this truth is so well\n",
    "      fixed in the minds of the surrounding families, that he is\n",
    "      considered the rightful property of some one or other of their\n",
    "      daughters.\n",
    "\n",
    "      “My dear Mr. Bennet,” said his lady to him one day, “have you\n",
    "      heard that Netherfield Park is let at last?”\n",
    "\n",
    "      Mr. Bennet replied that he had not.\n",
    "\n",
    "      “But it is,” returned she; “for Mrs. Long has just been here, and\n",
    "      she told me all about it.”\n",
    "\n",
    "      Mr. Bennet made no answer.\n",
    "\n",
    "      “Do you not want to know who has taken it?” cried his wife\n",
    "      impatiently.\n",
    "\n",
    "      “_You_ want to tell me, and I have no objection to hearing it.”\n",
    "\n",
    "      This was invitation enough.\n",
    "\n",
    "      “Why, my dear, you must know, Mrs. Long says that Netherfield is\n",
    "      taken by a young man of large fortune from the north of England;\n",
    "      that he came down on Monday in a chaise and four to see the\n",
    "      place, and was so much delighted with it, that he agreed with Mr.\n",
    "      Morris immediately; that he is to take possession before\n",
    "      Michaelmas, and some of his servants are to be in the house by\n",
    "      the end of next week.”\n",
    "\n",
    "      “What is his name?”\n",
    "\n",
    "      “Bingley.”\n",
    "\n",
    "      “Is he married or single?”\n",
    "\n",
    "      “Oh! Single, my dear, to be sure! A single man of large fortune;\n",
    "      four or five thousand a year. What a fine thing for our girls!”\n",
    "\n",
    "      “How so? How can it affect them?”\n",
    "\n",
    "      “My dear Mr. Bennet,” replied his wife, “how can you be so\n",
    "      tiresome! You must know that I am thinking of his marrying one of\n",
    "      them.”\n",
    "\n",
    "      “Is that his design in settling here?”\n",
    "\n",
    "      “Design! Nonsense, how can you talk so! But it is very likely\n",
    "      that he _may_ fall in love with one of them, and therefore you\n",
    "      must visit him as soon as he comes.”\n",
    "\n",
    "      “I see no occasion for that. You and the girls may go, or you may\n",
    "      send them by themselves, which perhaps will be still better, for\n",
    "      as you are as handsome as any of them, Mr. Bingley may like you\n",
    "      the best of the party.”\n",
    "\n",
    "      “My dear, you flatter me. I certainly _have_ had my share of\n",
    "      beauty, but I do not pretend to be anything extraordinary now.\n",
    "      When a woman has five grown-up daughters, she ought to give over\n",
    "      thinking of her own beauty.”\n",
    "\n",
    "      “In such cases, a woman has not often much beauty to think of.”\n",
    "\n",
    "      “But, my dear, you must indeed go and see Mr. Bingley when he\n",
    "      comes into the neighbourhood.”\n",
    "\n",
    "      “It is more than I engage for, I assure you.”\n",
    "\n",
    "      “But consider your daughters. Only think what an establishment it\n",
    "      would be for one of them. Sir William and Lady Lucas are\n",
    "      determined to go, merely on that account, for in general, you\n",
    "      know, they visit no newcomers. Indeed you must go, for it will be\n",
    "      impossible for _us_ to visit him if you do not.”\n",
    "\n",
    "      “You are over-scrupulous, surely. I dare say Mr. Bingley will be\n",
    "      very glad to see you; and I will send a few lines by you to\n",
    "      assure him of my hearty consent to his marrying whichever he\n",
    "      chooses of the girls; though I must throw in a good word for my\n",
    "      little Lizzy.”\n",
    "\n",
    "      “I desire you will do no such thing. Lizzy is not a bit better\n",
    "      than the others; and I am sure she is not half so handsome as\n",
    "      Jane, nor half so good-humoured as Lydia. But you are always\n",
    "      giving _her_ the preference.”\n",
    "\n",
    "      “They have none of them much to recommend them,” replied he;\n",
    "      “they are all silly and ignorant like other girls; but Lizzy has\n",
    "      something more of quickness than her sisters.”\n",
    "\n",
    "      “Mr. Bennet, how can you abuse your own children in such a way?\n",
    "      You take delight in vexing me. You have no compassion for my poor\n",
    "      nerves.”\n",
    "\n",
    "      “You mistake me, my dear. I have a high respect for your nerves.\n",
    "      They are my old friends. I have heard you mention them with\n",
    "      consideration these last twenty years at least.”\n",
    "\n",
    "      “Ah, you do not know what I suffer.”\n",
    "\n",
    "      “But I hope you will get over it, and live to see many young men\n",
    "      of four thousand a year come into the neighbourhood.”\n",
    "\n",
    "      “It will be no use to us, if twenty such should come, since you\n",
    "      will not visit them.”\n",
    "\n",
    "      “Depend upon it, my dear, that when there are twenty, I will\n",
    "      visit them all.”\n",
    "\n",
    "      Mr. Bennet was so odd a mixture of quick parts, sarcastic humour,\n",
    "      reserve, and caprice, that the experience of three-and-twenty\n",
    "      years had been insufficient to make his wife understand his\n",
    "      character. _Her_ mind was less difficult to develop. She was a\n",
    "      woman of mean understanding, little information, and uncertain\n",
    "      temper. When she was discontented, she fancied herself nervous.\n",
    "      The business of her life was to get her daughters married; its\n",
    "      solace was visiting and news.'''.lower()\n",
    "\n",
    "import re\n",
    "subs = '''_;.,”“?!'''\n",
    "for sub in subs:\n",
    "    prideAndPrejudiceFirstChapter = prideAndPrejudiceFirstChapter.replace(sub,' ')\n",
    "prideAndPrejudiceFirstChapter = re.sub('\\\\s+', ' ',prideAndPrejudiceFirstChapter).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_20988\\2791998106.py:23: RuntimeWarning: invalid value encountered in divide\n",
      "  transition_matrix[i, :] /= transition_matrix[i, :].sum()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.        , 0.        , 0.        , ..., 0.        , 0.04761905,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         1.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.03225806, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]]),\n",
       " 129,\n",
       " 0.2857142857142857)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Part 1, find the words by splitting prideAndPrejudiceFirstChapter on ' ',\n",
    "words = prideAndPrejudiceFirstChapter.split(' ')\n",
    "unique_words = sorted(set(words)) # The unique words\n",
    "n_words = len(unique_words) # The number of unique words\n",
    "\n",
    "# Part 2, count the different transitions\n",
    "transitions = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
    "transition_counts = makeFreqDict(transitions)\n",
    "indexToWord = {i: word for i, word in enumerate(unique_words)}\n",
    "wordToIndex = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "\n",
    "\n",
    "# Part 3, finding the maximum likelihood estimate of the transition matrix\n",
    "transition_matrix = np.zeros((n_words, n_words))\n",
    "for (word1, word2), count in transition_counts.items():\n",
    "   \n",
    "    i, j = wordToIndex[word1], wordToIndex[word2]\n",
    "    transition_matrix[i, j] = count\n",
    "\n",
    "# Normalize the rows of the transition matrix\n",
    "for i in range(n_words):\n",
    "    transition_matrix[i, :] /= transition_matrix[i, :].sum()\n",
    "    \n",
    "transition_matrix,wordToIndex['is'],transition_matrix[130,129]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "4",
    "lx_problem_points": "5"
   },
   "outputs": [],
   "source": [
    "# Part 1, find the words by splitting prideAndPrejudiceFirstChapter on ' ',\n",
    "# Make sure you ran the cell above before you try this\n",
    "words = XXX\n",
    "unique_words = sorted(set(words)) # The unique words\n",
    "n_words = len(unique_words) # The number of unique words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "4",
    "lx_problem_points": "5"
   },
   "outputs": [],
   "source": [
    "# Part 2, count the different transitions\n",
    "transitions = XXX # A list containing tuples ex: ('it','is') of all transitions in the text\n",
    "transition_counts = XXX # A dictionary that counts the number of each transition \n",
    "# ex: ('it','is'):4\n",
    "indexToWord = XXX # A dictionary that maps the n-1 number to the n:th unique_word,\n",
    "# ex: 0:'a'\n",
    "wordToIndex = XXX # The inverse function of indexToWord, \n",
    "# ex: 'a':0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "4",
    "lx_problem_points": "5"
   },
   "outputs": [],
   "source": [
    "# Part 3, finding the maximum likelihood estimate of the transition matrix\n",
    "import numpy as np\n",
    "\n",
    "transition_matrix = XXX # a numpy array of size (n_words,n_words)\n",
    "\n",
    "# The transition matrix should be ordered in such a way that\n",
    "# p_{'it','is'} = transition_matrix[wordToIndex['it'],wordToIndex['is']]\n",
    "\n",
    "# Make sure that the transition_matrix does not contain np.nan from division by zero for instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "Test",
    "lx_problem_number": "4",
    "lx_problem_points": "5"
   },
   "source": [
    "---\n",
    "#### Local Test for PROBLEM 4\n",
    "Use the cell below to evaluate the feasibility of your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "Test",
    "lx_problem_number": "4",
    "lx_problem_points": "5"
   },
   "outputs": [],
   "source": [
    "# Once you have created all your functions, you can make a small test here to see\n",
    "# what would be generated from your model.\n",
    "\n",
    "start = np.zeros(shape=(n_words,1))\n",
    "start[0,0] = 1\n",
    "\n",
    "current_pos = start\n",
    "for i in range(100):\n",
    "    random_word_index = np.random.choice(range(n_words),p=current_pos.reshape(-1))\n",
    "    current_pos = np.zeros_like(start)\n",
    "    current_pos[random_word_index] = 1\n",
    "    print(indexToWord[random_word_index],end=' ')\n",
    "    current_pos = (current_pos.T@transition_matrix).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "5",
    "lx_problem_points": "5"
   },
   "source": [
    "---\n",
    "## PROBLEM 5\n",
    "Maximum Points = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "5",
    "lx_problem_points": "5"
   },
   "source": [
    "\n",
    "1. [1p] Draw a uniform random point $X$ on the surface of the unit sphere in $\\mathbb{R}^d$. What is the variance of $X_1$ (the first coordinate)? Solve this using pen and paper, then fill in the answer below in `variance_x1_problem7`.\n",
    "2. [1p] How large must $\\epsilon$ be for $99\\%$ of the volume of a $d$-dimensional unit-radius ball to lie in the shell of $\\epsilon$-thickness at the surface of the ball?\n",
    "3. [3p] The volume of the unit ball is given by\n",
    "$$\n",
    "    V(d) = \\frac{2 \\pi^{\\frac{d}{2}}}{d \\Gamma(\\frac{d}{2})} = \\frac{ \\pi^{\\frac{d}{2}}}{ (\\frac{d}{2})!}\n",
    "$$\n",
    "What function of $d$ would the radius need to be for a ball or radius $r$ to have approximately constant volume as a function of $d$? Hint use Stirlings formula $n! \\approx (n/e)^n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = var('d')\n",
    "# Part1, what is the value of the variance for problem 1\n",
    "variance_x1_problem7 = 1/(d) # Fill this as a function of d (sagemath symbolic expression)\n",
    "\n",
    "# Part 2, what is the value of epsilon for question 2\n",
    "epsilon = solve((1 - (1 - (epsilon^2/2))^d) == 0.01, epsilon)\n",
    "\n",
    "# Part 3, what is the radius from problem 3\n",
    "r = solve(V == pi^(d/2)/gamma(d/2+1), r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "5",
    "lx_problem_points": "1"
   },
   "outputs": [],
   "source": [
    "# Part1, what is the value of the variance for problem 1\n",
    "d = var('d')\n",
    "# Use exact expression, use rationals and not 1.0\n",
    "variance_x1_problem7 = XXX # Fill this as a function of d (sagemath symbolic expression)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "5",
    "lx_problem_points": "1"
   },
   "outputs": [],
   "source": [
    "# Part 2, what is the value of epsilon for question 2\n",
    "d = var('d')\n",
    "# Use exact expression, use rationals and not 1.0\n",
    "epsilon = XXX # Fill this as a function of d (sagemath symbolic expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "5",
    "lx_problem_points": "3"
   },
   "outputs": [],
   "source": [
    "# Part 3, what is the radius from problem 3\n",
    "d = var('d')\n",
    "# Use exact expression, use rationals and not 1.0.\n",
    "r = XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "6",
    "lx_problem_points": "5"
   },
   "source": [
    "---\n",
    "## PROBLEM 6\n",
    "Maximum Points = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "6",
    "lx_problem_points": "5"
   },
   "source": [
    "\n",
    "Consider the data `X` and `y`, in the cell below. `X` denotes $20$ points in $\\mathbb{R}^2$ and `y` corresponds to the labels for these points, i.e. it is a classification problem.\n",
    "\n",
    "1. Implement the function `perceptron` by filling in `XXX`.\n",
    "2. Use your implemented `perceptron` function to compute a vector (numpy array) $\\hat w$ with shape `(3,1)` such that \n",
    "$$\n",
    "    (\\hat w \\cdot \\hat x_i) l_i > 0, \\quad \\forall i=1,\\ldots,20\n",
    "$$\n",
    "put your answer in `hat_w` below\n",
    "3. Use the vector $\\hat w$ that you just found and compute $r$ (put your result in `r`), finally use this to give an upper bound to the number of iterations needed for the perceptron algorithm to converge on this dataset, see the Theorem in notebook 15. Put the result in `iteration_bound`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "6",
    "lx_problem_points": "5"
   },
   "outputs": [],
   "source": [
    "\n",
    "X = np.array([[0.14774693918368506,0.8537253157278155],[-0.1755517430286779,0.8979710703337818],[0.5227216475286975,0.7448281947022451],[-0.5071170511153492,0.8002027400836075],[-0.39436968212400453,1.0177689414422981],[-0.3983065780966649,1.0443663197782966],[-0.08652771617599643,0.48036820824519255],[0.15352541170101042,0.6820807981911706],[-0.3303348532791869,1.120673883903539],[-0.2656220857139274,0.8526638282828739],[0.7259603693529442,0.25428467532034965],[0.4577253912481767,-0.2358809079980879],[0.9722462145222105,0.13128550836973255],[0.4089349951770505,-0.09503914544452634],[0.9718156747909192,0.3524307824261209],[1.2009353774940565,-0.25004126389987974],[1.271791635779178,-0.07571928320750206],[0.36784476124502913,-0.23743021661715671],[0.8918396050420891,-0.1029336332277948],[0.4501578013678095,-0.13188266835015783]])+np.array([10,0]).reshape(1,-1)\n",
    "y = np.array([1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ASSIGNMENT 2通过版\n",
    "\n",
    "# Part 1\n",
    "\n",
    "def perceptron(X_in, labels, max_iter=1000):\n",
    "    '''Runs the perceptron algorithm on X_in, labels, and does a maximum of max_iter iterations'''\n",
    "    # Adding the extra dimension with value 1 for the bias\n",
    "    X = np.c_[X_in, np.ones(X_in.shape[0])]\n",
    "    # Initialize the weight vector w with zeros. Length is number of features + 1 (for bias)\n",
    "    w = np.zeros(X.shape[1])\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        for i in range(len(X)):\n",
    "            # Update the weight vector if a misclassification occurs\n",
    "            if (np.dot(w, X[i]) * labels[i]) <= 0:\n",
    "                w = w + labels[i] * X[i]\n",
    "    return w  # w should have the shape (number of features + 1)\n",
    "\n",
    "# Example usage with the previously provided dataset\n",
    "# Note: This is just for demonstration and won't be run here as it's intended to show usage.\n",
    "\n",
    "# Part 2\n",
    "hat_w= perceptron(X, y)\n",
    "\n",
    "# Part 3\n",
    "X_=np.c_[X, np.ones(X.shape[0])]\n",
    "r = np.max(np.linalg.norm(X_, axis=1))\n",
    "\n",
    "gamma=np.min(y*(np.dot(X_,hat_w)))\n",
    "positive_values = y * (np.dot(X_, hat_w))\n",
    "\n",
    "iteration_bound =(((r * np.linalg.norm(hat_w))**2 ) / (gamma)**2)\n",
    "\n",
    "print(hat_w,r,iteration_bound)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "6",
    "lx_problem_points": "3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hat_w: [-1.49468903 34.08611021  3.        ]\n",
      "34.25050370930761 1.6398625226716668\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Part 1\n",
    "def perceptron(X_in, labels, max_iter=1000):\n",
    "    w = np.zeros(X_in.shape[1]+1)\n",
    "    for _ in range(max_iter):\n",
    "        for i in range(X_in.shape[0]):\n",
    "            if labels[i] * (np.dot(w, np.append(X_in[i], 1))) <= 0:\n",
    "                w += labels[i] * np.append(X_in[i], 1)\n",
    "                break\n",
    "    return w\n",
    "\n",
    "hat_w = perceptron(X, y)\n",
    "\n",
    "print(\"hat_w:\", hat_w)\n",
    "\n",
    "# Part 2\n",
    "r = np.linalg.norm(hat_w)\n",
    "iteration_bound = (r**2) * (max(y * (X @ hat_w[:-1] + hat_w[-1])))**(-2)\n",
    "print(r,iteration_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "6",
    "lx_problem_points": "2"
   },
   "outputs": [],
   "source": [
    "# Part 1\n",
    "def perceptron(X_in,labels,max_iter=1000):\n",
    "    '''Runs the perceptron algorithm on X_in, labels, and does a maximum of max_iter updates'''\n",
    "    w = XXX\n",
    "    \n",
    "    return w #Make sure that w has the shape described in the problem\n",
    "\n",
    "hat_w = XXX\n",
    "# Part 2\n",
    "\n",
    "r = XXX\n",
    "\n",
    "iteration_bound = XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "7",
    "lx_problem_points": "5"
   },
   "source": [
    "---\n",
    "## PROBLEM 7\n",
    "Maximum Points = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "7",
    "lx_problem_points": "5"
   },
   "source": [
    "\n",
    "Perform a bootstrap to find the plug-in estimate and 99% CI for the 95-th Percentile of the inter-EQ time in minutes.\n",
    "\n",
    "You just need to evaluate the next `REQUIRED-CELL` and replace `XXX` with the right expressions in the following cell.\n",
    "\n",
    "NOTE: If `data/earthquakes.csv` is not available and you get a file not found when evaluating the next `REQUIRED-CELL`, you can get the csv by `unzip` as follows:\n",
    "\n",
    "```\n",
    "%%sh\n",
    "cd data\n",
    "unzip earthquakes.csv.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "7",
    "lx_problem_points": "5"
   },
   "outputs": [],
   "source": [
    "# REQUIRED-CELL\n",
    "# DO NOT MODIFY this cell \n",
    "# Evaluate this cell before trying this PROBLEM so that the required functions and variables are loaded\n",
    "\n",
    "import numpy as np\n",
    "## Be Patient! - This will take more time, about a minute or so\n",
    "###############################################################################################\n",
    "def getLonLatMagDepTimes(NZEQCsvFileName):\n",
    "    '''returns longitude, latitude, magnitude, depth and the origin time as unix time\n",
    "    for each observed earthquake in the csv filr named NZEQCsvFileName'''\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    from dateutil.parser import parse\n",
    "    import numpy as np\n",
    "    \n",
    "    with open(NZEQCsvFileName) as f:\n",
    "        reader = f.read() \n",
    "        dataList = reader.split('\\n')\n",
    "        \n",
    "    myDataAccumulatorList =[]\n",
    "    for data in dataList[1:-1]:\n",
    "        dataRow = data.split(',')\n",
    "        myTimeString = dataRow[2] # origintime\n",
    "        # let's also grab longitude, latitude, magnitude, depth\n",
    "        myDataString = [dataRow[4],dataRow[5],dataRow[6],dataRow[7]]\n",
    "        try: \n",
    "            myTypedTime = time.mktime(parse(myTimeString).timetuple())\n",
    "            myFloatData = [float(x) for x in myDataString]\n",
    "            myFloatData.append(myTypedTime) # append the processed timestamp\n",
    "            myDataAccumulatorList.append(myFloatData)\n",
    "        except TypeError as e: # error handling for type incompatibilities\n",
    "            print ('Error:  Error is ', e)\n",
    "    #return np.array(myDataAccumulatorList)\n",
    "    return myDataAccumulatorList\n",
    "\n",
    "myProcessedList = getLonLatMagDepTimes('data/earthquakes.csv')\n",
    "\n",
    "def interQuakeTimes(quakeTimes):\n",
    "    '''Return a list inter-earthquake times in seconds from earthquake origin times\n",
    "    Date and time elements are expected to be in the 5th column of the array\n",
    "    Return a list of inter-quake times in seconds. NEEDS sorted quakeTimes Data'''\n",
    "    import numpy as np\n",
    "    retList = []\n",
    "    if len(quakeTimes) > 1:\n",
    "        retList = [quakeTimes[i]-quakeTimes[i-1] for i in range(1,len(quakeTimes))]\n",
    "    #return np.array(retList)\n",
    "    return retList\n",
    "\n",
    "def makeBootstrappedConfidenceIntervalOfStatisticT(dataset, statT, alpha, B=100):\n",
    "    '''make a bootstrapped 1-alpha confidence interval for ANY given statistic statT \n",
    "    from the dataset with B Bootstrap replications for 0 < alpha < 1, and \n",
    "    return lower CI, upper CI, bootstrapped_samples '''\n",
    "    n = len(dataset) # sample size of the original dataset\n",
    "    bootstrappedStatisticTs=[] # list to store the statistic T from each bootstrapped data\n",
    "    for b in range(B):\n",
    "        #sample indices at random between 0 and len(iQMinutes)-1 to make the bootstrapped dataset\n",
    "        randIndices=[randint(0,n-1) for i in range(n)] \n",
    "        bootstrappedDataset = dataset[randIndices] # resample with replacement from original dataset\n",
    "        bootstrappedStatisticT = statT(bootstrappedDataset)\n",
    "        bootstrappedStatisticTs.append(bootstrappedStatisticT)\n",
    "    # noe get the [2.5%, 97.5%] percentile-based CI\n",
    "    alpaAsPercentage=alpha*100.0\n",
    "    lowerBootstrap1MinusAlphaCIForStatisticT = np.percentile(bootstrappedStatisticTs,alpaAsPercentage/2)\n",
    "    upperBootstrap1MinusAlphaCIForStatisticT = np.percentile(bootstrappedStatisticTs,100-alpaAsPercentage/2)\n",
    "    return (lowerBootstrap1MinusAlphaCIForStatisticT,upperBootstrap1MinusAlphaCIForStatisticT,\\\n",
    "            np.array(bootstrappedStatisticTs))\n",
    "\n",
    "interQuakesSecs = interQuakeTimes(sorted([x[4] for x in myProcessedList]))\n",
    "iQMinutes = np.array(interQuakesSecs)/60.0\n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "7",
    "lx_problem_points": "5"
   },
   "outputs": [],
   "source": [
    "# replace XXX with the right expressions\n",
    "statT95thPercentile = lambda dataset : np.percentile(dataset, 95) #statistic of interest (dataset is an np.array)\n",
    "alpha= 0.01\n",
    "B=1000 # number of bootstrap samples, reduce this to 100 while debuging and back to 1000 when done\n",
    "# plug-in point estimate of the 75th-Percentile of inter-EQ Times\n",
    "plugInEstimateOf95thPercentile = statT95thPercentile(iQMinutes)\n",
    "# get the bootstrapped samples and build 1-alpha confidence interval\n",
    "# do NOT change anything below\n",
    "lowerCIT95P,upperCIT95P,bootValuesT95P = \\\n",
    "                      makeBootstrappedConfidenceIntervalOfStatisticT(iQMinutes, statT95thPercentile, alpha, B)\n",
    "print (\"The Plug-in Point Estimate of the 95th-Percentile of inter-EQ Times = \", plugInEstimateOf95thPercentile)\n",
    "print (\"1-alpha Bootstrapped CI for the 95th-Percentile of inter-EQ Times = \",(lowerCIT95P,upperCIT95P))\n",
    "#print (\"         for alpha = \",alpha.n(digits=2),\" and bootstrap replicates = \",B)\n",
    "print(\"         for alpha = {:.2f} and bootstrap replicates = {}\".format(alpha, B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "8",
    "lx_problem_points": "5"
   },
   "source": [
    "---\n",
    "## PROBLEM 8\n",
    "Maximum Points = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "8",
    "lx_problem_points": "5"
   },
   "source": [
    "\n",
    "Consider $n$ IID samples from a continuous random variable with the following probability density function:\n",
    "$$\n",
    "f(x; \\beta) = \\frac{x}{\\beta^2} \\exp\\left(-\\frac{1}{2}(x/\\beta)^2\\right), \\qquad \\text{ where, } \\beta>0, x \\geq 0\n",
    "$$\n",
    "Use Bounded 1D Optimisation to find the maximum likelihood estimate for the IID experiment above using the dataset which is given in the numpy array `dataSamplesForProblem1` below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "0",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "8",
    "lx_problem_points": "5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "dataSamples1 = np.array([2.30, 4.10, 3.60, 2.50, 3.20, 1.90, 2.60, 1.50, 2.80, 2.90])\n",
    "\n",
    "# finding MLE numerically for parameter beta - replace XXX by the right expression\n",
    "# do NOT change the function name `negLogLklOfIIDSamplesInProblem1or2`\n",
    "data = dataSamples1\n",
    "def negLogLklOfIID1(paramBeta,data):\n",
    "    '''negative log likelihood function for IID trials in Problem 1 or 2'''\n",
    "    if paramBeta <= 0:\n",
    "        return np.inf\n",
    "    return np.sum(np.log(paramBeta**2) + 0.5 * (data / paramBeta)**2 - np.log(data))\n",
    "\n",
    "# you should NOT change variable names - just replace XXX\n",
    "boundedResult1 = optimize.minimize_scalar(negLogLklOfIID1, args=(dataSamples1,), bounds=(0.01, 10), method='bounded')\n",
    "boundedResult1\n",
    "\n",
    "\n",
    "# finding MLE numerically for parameter beta\n",
    "def negLogLklOfIID1(paramBeta):\n",
    "    return -np.sum(np.log(dataSamples1/paramBeta**2) - 0.5*(dataSamples1/paramBeta)**2)\n",
    "\n",
    "boundedResult1 = optimize.minimize_scalar(negLogLklOfIID1, bounds=(1e-5, 1e5), method='bounded')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "lx_assignment_number": "[8,2,7,3,4,6,5,1]",
  "lx_course_instance": "2020",
  "lx_course_name": "Introduction to Data Science: A Comp-Math-Stat Approach",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
